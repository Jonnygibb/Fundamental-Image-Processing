{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WM391 PMA Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network for the Exposure Correction of Poorly Exposed Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a Generative Adversarial Network for the purpose of generating well exposed images using training data based on images which are over or over exposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import albumentations as a\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters for the use of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration is setup to use the WM391_PMA_dataset. Use with other datasets will require modification of the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chooses the most appropriate device given the machines constraints\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Set path of dataset. Please change as appripriate\n",
    "TRAIN_DIR = \"WM391_PMA_dataset\\\\training\"\n",
    "VAL_DIR = \"WM391_PMA_dataset\\\\validation\"\n",
    "# Determines how quickly the gradient is travelled for the machine learning model\n",
    "LEARNING_RATE = 2e-4\n",
    "# Sets the number of images that are sent to the device per iteration\n",
    "BATCH_SIZE = 8\n",
    "# Number of cpu threads used\n",
    "NUM_WORKERS = 2\n",
    "# Size of the images used to train the model\n",
    "IMAGE_SIZE = 256\n",
    "# Specifies the number of channels in the images input to the model\n",
    "CHANNELS_IMG = 3\n",
    "L1_LAMBDA = 100\n",
    "LAMBDA_GP = 10\n",
    "# Number of times the model is trained with the entire training dataset\n",
    "NUM_EPOCHS = 10\n",
    "# Load model weights & parameters from checkpoint state\n",
    "LOAD_MODEL = False\n",
    "# Save model weights & parameters to checkpoint file\n",
    "SAVE_MODEL = True\n",
    "# Set file location for the discriminator and generator checkpoint files\n",
    "CHECKPOINT_DISC = \"disc.pth.tar\"\n",
    "CHECKPOINT_GEN = \"gen.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Image Transformations\n",
    "\n",
    "Apply a crop to all images to train the model with a consistent image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_transform = a.Compose(\n",
    "    [a.Resize(width=256, height=256),], additional_targets={\"image0\": \"image\"},\n",
    ")\n",
    "\n",
    "transform_varied_exposure = a.Compose(\n",
    "    [\n",
    "        a.HorizontalFlip(p=0.5),\n",
    "        a.ColorJitter(p=0.2),\n",
    "        a.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_ground_truth = a.Compose(\n",
    "    [\n",
    "        a.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the poorly exposed image and its corresponding ground truth image. This gives the GAN an input image and a target image to work towards for every example in the training dataset. Using pytorch dataloaders requries overwriting of the datasets ```__len__``` and the ```__getitem__``` methods to return the correct size of the dataset and an example from the dataset respectivly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExposedImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform_both=None, transform_varied_exposure=None, transform_ground_truth=None):\n",
    "        \n",
    "        # Set paths to image directories\n",
    "        self.root_dir = root_dir\n",
    "        self.variable_exposure_path = os.path.join(root_dir, \"INPUT_IMAGES\")\n",
    "        self.ground_truth_path = os.path.join(root_dir, \"GT_IMAGES\")\n",
    "\n",
    "        # Initialise transforms to class variables\n",
    "        self.transform_both = transform_both\n",
    "        self.transform_varied_exposure = transform_varied_exposure\n",
    "        self.transform_ground_truth = transform_ground_truth\n",
    "\n",
    "        # Get the list of file names from the directories\n",
    "        self.variable_exposure_images = os.listdir(self.variable_exposure_path)\n",
    "        self.ground_truth_images = os.listdir(self.ground_truth_path)\n",
    "        \n",
    "        # Get length of individual dataset classes\n",
    "        self.variable_exposure_len = len(self.variable_exposure_images)\n",
    "        self.ground_truth_len = len(self.ground_truth_images)\n",
    "        \n",
    "        # Use the variable exposure length since it holds all the training images\n",
    "        self.length_dataset = self.variable_exposure_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def get_file_name(self, index):\n",
    "        # Modulo input index to prevent an index out of range of the dataset\n",
    "        index = index % self.length_dataset\n",
    "        variable_exposure_image = self.variable_exposure_images[index]\n",
    "        # Floor the ground truth index by 5 since there are 5 exposures for every corresponding ground truth\n",
    "        ground_truth_image = self.ground_truth_images[index // 5]\n",
    "\n",
    "        return variable_exposure_image, ground_truth_image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        variable_exposure_image, ground_truth_image = self.get_file_name(index)\n",
    "\n",
    "        # Create full path to image\n",
    "        variable_exposure_image_path = os.path.join(self.variable_exposure_path, variable_exposure_image)\n",
    "        ground_truth_image_path = os.path.join(self.ground_truth_path, ground_truth_image)\n",
    "\n",
    "        # Open the image as an RGB numpy array\n",
    "        variable_exposure_image = np.array(Image.open(variable_exposure_image_path).convert(\"RGB\"))\n",
    "        ground_truth_image = np.array(Image.open(ground_truth_image_path).convert(\"RGB\"))\n",
    "\n",
    "        # If there's an image transform for both images, apply the transform\n",
    "        if self.transform_both:\n",
    "            augentations = self.transform_both(image=variable_exposure_image, image0=ground_truth_image)\n",
    "            variable_exposure_image = augentations[\"image\"]\n",
    "            ground_truth_image = augentations[\"image0\"]\n",
    "\n",
    "        # If ther's an image transform for the varied exposure image, apply the transform\n",
    "        if self.transform_varied_exposure:\n",
    "            variable_exposure_image = self.transform_varied_exposure(image=variable_exposure_image)[\"image\"]\n",
    "\n",
    "        # IF there's an image transform for the ground truth image, apply the transform\n",
    "        if self.transform_ground_truth:\n",
    "            ground_truth_image = self.transform_varied_exposure(image=ground_truth_image)[\"image\"]\n",
    "\n",
    "        return variable_exposure_image, ground_truth_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Exposed Image Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet should return 5 image pairs (1 varied exposure and 1 ground truth). If everything is working correctly, the varied exposure and ground truth should produce tensors of size 256x256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable exposure: torch.Size([5, 3, 256, 256])\n",
      "Ground truth: torch.Size([5, 3, 256, 256])\n",
      "Variable exposure: torch.Size([5, 3, 256, 256])\n",
      "Ground truth: torch.Size([5, 3, 256, 256])\n",
      "Variable exposure: torch.Size([5, 3, 256, 256])\n",
      "Ground truth: torch.Size([5, 3, 256, 256])\n",
      "Variable exposure: torch.Size([5, 3, 256, 256])\n",
      "Ground truth: torch.Size([5, 3, 256, 256])\n",
      "Variable exposure: torch.Size([5, 3, 256, 256])\n",
      "Ground truth: torch.Size([5, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dataset = ExposedImageDataset(\n",
    "    TRAIN_DIR,\n",
    "    transform_both=both_transform,\n",
    "    transform_varied_exposure=transform_varied_exposure,\n",
    "    transform_ground_truth=transform_ground_truth\n",
    "    )\n",
    "loader = DataLoader(dataset, batch_size=5)\n",
    "count = 0\n",
    "for x, y in loader:\n",
    "    if(count < 5):\n",
    "        print(\"Variable exposure: {}\".format(x.shape))\n",
    "        print(\"Ground truth: {}\".format(y.shape))\n",
    "    else:\n",
    "        break\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions to make model easier to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_examples(gen, val_loader, epoch, folder):\n",
    "    x, y = next(iter(val_loader))\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        y_fake = gen(x)\n",
    "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
    "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
    "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
    "        if epoch == 1:\n",
    "            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n",
    "    gen.train()\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model for Discriminator\n",
    "\n",
    "The Discriminator's job is to decide whether an image is real or fake. The learning from the discriminator is used to feedback to the generator to improve its ability to make images more like the target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels * 2,\n",
    "                features[0],\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2),\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = self.initial(x)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (initial): Sequential(\n",
      "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (model): Sequential(\n",
      "    (0): CNNBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (1): CNNBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (2): CNNBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=reflect)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2)\n",
      "      )\n",
      "    )\n",
      "    (3): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1, 3, 256, 256))\n",
    "y = torch.randn((1, 3, 256, 256))\n",
    "model = Discriminator(in_channels=3)\n",
    "preds = model(x, y)\n",
    "print(model)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model for the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.down = down\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.dropout(x) if self.use_dropout else x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down2 = Block(\n",
    "            features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down3 = Block(\n",
    "            features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down4 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down5 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down6 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(features * 8, features * 8, 4, 2, 1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up2 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
    "        )\n",
    "        self.up3 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
    "        )\n",
    "        self.up4 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up5 = Block(\n",
    "            features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up6 = Block(\n",
    "            features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, in_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.initial_down(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        d6 = self.down5(d5)\n",
    "        d7 = self.down6(d6)\n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        up1 = self.up1(bottleneck)\n",
    "        up2 = self.up2(torch.cat([up1, d7], 1))\n",
    "        up3 = self.up3(torch.cat([up2, d6], 1))\n",
    "        up4 = self.up4(torch.cat([up3, d5], 1))\n",
    "        up5 = self.up5(torch.cat([up4, d4], 1))\n",
    "        up6 = self.up6(torch.cat([up5, d3], 1))\n",
    "        up7 = self.up7(torch.cat([up6, d2], 1))\n",
    "        return self.final_up(torch.cat([up7, d1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1, 3, 256, 256))\n",
    "model = Generator(in_channels=3, features=64)\n",
    "preds = model(x)\n",
    "print(preds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def train_fn(disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, g_scaler, d_scaler,):\n",
    "    \n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        # Train Discriminator\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_fake = gen(x)\n",
    "            D_real = disc(x, y)\n",
    "            D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
    "            D_fake = disc(x, y_fake.detach())\n",
    "            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n",
    "            D_loss = (D_real_loss + D_fake_loss) / 2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train generator\n",
    "        with torch.cuda.amp.autocast():\n",
    "            D_fake = disc(x, y_fake)\n",
    "            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
    "            L1 = l1_loss(y_fake, y) * L1_LAMBDA\n",
    "            G_loss = G_fake_loss + L1\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            loop.set_postfix(\n",
    "                D_real=torch.sigmoid(D_real).mean().item(),\n",
    "                D_fake=torch.sigmoid(D_fake).mean().item(),\n",
    "            )\n",
    "\n",
    "\n",
    "def main():\n",
    "    disc = Discriminator(in_channels=3).to(DEVICE)\n",
    "    gen = Generator(in_channels=3, features=64).to(DEVICE)\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    BCE = nn.BCEWithLogitsLoss()\n",
    "    L1_LOSS = nn.L1Loss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    train_dataset = ExposedImageDataset(root_dir=TRAIN_DIR)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "    val_dataset = ExposedImageDataset(root_dir=VAL_DIR)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(\n",
    "            disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler,\n",
    "        )\n",
    "\n",
    "        if SAVE_MODEL and epoch % 5 == 0:\n",
    "            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "            save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n",
    "\n",
    "        save_examples(gen, val_loader, epoch, folder=\"evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2210 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79c4193ea2437410cb5e73be0327dd99929b50b42a9030e9ed553799e75925c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('WM391')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
